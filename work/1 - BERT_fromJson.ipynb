{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dorukhan/anaconda2/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dorukhan/anaconda2/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dorukhan/anaconda2/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dorukhan/anaconda2/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dorukhan/anaconda2/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dorukhan/anaconda2/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/dorukhan/anaconda2/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dorukhan/anaconda2/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dorukhan/anaconda2/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dorukhan/anaconda2/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dorukhan/anaconda2/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dorukhan/anaconda2/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer,BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from typing import List, Any\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main.js  mainWindow.html  package.json\tpackage-lock.json  README.md\n"
     ]
    }
   ],
   "source": [
    "! cd ../labeltool_extractionbased/;ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example.json','rb') as f:\n",
    "    jfile  = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonFileTokenizer():\n",
    "    \n",
    "    def __init__(self,jsonFile,tokenizer,forEval=False):\n",
    "        \n",
    "        self.jsonFile = jsonFile\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = None\n",
    "        self.forEval = forEval\n",
    "        \n",
    "        self.len_text = None\n",
    "        self.max_len = 0\n",
    "        \n",
    "        self.sentences = None\n",
    "        self.tokenized_sentences = []\n",
    "        self.token_ids = []\n",
    "        self.token_segments = []\n",
    "        \n",
    "        self.tensor_pairs = []\n",
    "        self.padded_sequence_tensor = None\n",
    "        self.segments_tensor = None\n",
    "        \n",
    "   \n",
    "    def read_json(self):\n",
    "        \n",
    "        with open(self.jsonFile,'rb') as f:\n",
    "            jfile  = json.load(f)\n",
    "            \n",
    "        sents = [sentence['content'] for sentence in jfile['sentences']] \n",
    "        \n",
    "        if self.forEval:\n",
    "            self.labels = np.array([sentence['deletedInRound'] for sentence in jfile['sentences']])\n",
    "            \n",
    "        self.sentences = sents  \n",
    "        self.len_text = len(sents)\n",
    "        \n",
    "        return sents\n",
    "\n",
    "    \"\"\"\n",
    "    Add special tokens for BERT\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def add_special_tokens(sentence):\n",
    "        \n",
    "        sentence = '[CLS] ' + sentence + ' [SEP]'\n",
    "        \n",
    "        return sentence\n",
    "   \n",
    "        \n",
    "    \"\"\"\n",
    "    Tokenize Single Sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    def tokenize_single_sentence(self,sentence):\n",
    "        \n",
    "        sentenceForBert = JsonFileTokenizer.add_special_tokens(sentence)\n",
    "        token_list = self.tokenizer.tokenize(sentenceForBert)\n",
    "        \n",
    "        return token_list\n",
    "    \n",
    "    \"\"\"\n",
    "    Tokenize Sentences by Iterating over them\n",
    "    \"\"\"\n",
    "    def tokenize_text(self):\n",
    "        self.read_json()\n",
    "        for sent in self.sentences:\n",
    "            tokenized_sentence = self.tokenize_single_sentence(sent)\n",
    "            if len(tokenized_sentence)>self.max_len:\n",
    "                self.max_len = len(tokenized_sentence)\n",
    "            self.tokenized_sentences.append(tokenized_sentence)\n",
    "            \n",
    "            \n",
    "        return self.tokenized_sentences\n",
    "    \n",
    "    def token_to_ids(self,output=False):\n",
    "        self.tokenize_text()\n",
    "        for token_list in self.tokenized_sentences:\n",
    "            id_list = self.tokenizer.convert_tokens_to_ids(token_list)\n",
    "            segment_list = [1]*len(id_list)\n",
    "            self.token_ids.append(id_list)\n",
    "            self.token_segments.append(segment_list)\n",
    "        \n",
    "        if output:\n",
    "            return self.token_ids\n",
    "        \n",
    "   \n",
    "        \n",
    "    def prepare_for_single_inference(self,output=False):\n",
    "        self.clear_state()\n",
    "        self.token_to_ids()\n",
    "        assert len(self.token_ids) == len(self.token_segments)\n",
    "        \n",
    "        for tokens,segments in zip(self.token_ids,self.token_segments):\n",
    "            token_tensor = torch.tensor([tokens])\n",
    "            segment_tensor = torch.tensor([segments])\n",
    "            self.tensor_pairs.append((token_tensor,segment_tensor))\n",
    "            \n",
    "        if output:\n",
    "            return self.tensor_pairs\n",
    "    \n",
    "    \n",
    "    def prepare_for_batch_inference(self):\n",
    "        self.prepare_for_single_inference()\n",
    "        token_tensor_list = [x[0].T for x in self.tensor_pairs]\n",
    "        self.padded_sequence_tensor = pad_sequence(token_tensor_list).T\n",
    "        self.segments_tensor = torch.ones(self.padded_sequence_tensor.shape)\n",
    "        \n",
    "        return self.padded_sequence_tensor, self.segments_tensor\n",
    "    \n",
    "    \n",
    "    def clear_state(self):\n",
    "        \n",
    "        self.len_text = None\n",
    "        self.max_len = 0\n",
    "        \n",
    "        self.sentences = None\n",
    "        self.tokenized_sentences = []\n",
    "        self.token_ids = []\n",
    "        self.token_segments = []\n",
    "        \n",
    "        self.tensor_pairs = []\n",
    "        self.padded_sequence_tensor = None\n",
    "        self.segments_tensor = None\n",
    "        \n",
    "        \n",
    "def select_layer(bertOut: tuple,layers: List[int],return_cls: Any) -> np.ndarray:\n",
    "    \n",
    "    \"\"\"\n",
    "    Selects and averages layers from BERT output\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    bertOut: tuple\n",
    "    Tuple containing output of 12 intermediate layers after feeding a document.\n",
    "    \n",
    "    layers: List of integers\n",
    "    List that contains which layer to choose. max = 11, min = 0.\n",
    "    \n",
    "    return_cls: bool\n",
    "    Whether to use CLS token embedding as sentence embedding instead of averaging token embeddings.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    numpy.ndarray (n_sentences, embedding_size) Embedding size if default to 768.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_layers = len(layers)\n",
    "    n_sentences = bertOut[0].shape[0]\n",
    "    n_tokens = bertOut[0].shape[1]\n",
    "    \n",
    "    assert min(layers) > -1 \n",
    "    assert max(layers) < 12\n",
    "    \n",
    "    if return_cls:\n",
    "        cls_matrix = np.zeros((n_layers,n_sentences,768))\n",
    "        l_ix = 0\n",
    "        for l,layer in enumerate(bertOut):\n",
    "            if l not in layers:\n",
    "                continue\n",
    "            else:\n",
    "                l_ix=l_ix+1\n",
    "            for s,sentence in enumerate(layer):\n",
    "                cls_tensor = sentence[0].numpy()\n",
    "                cls_matrix[l_ix-1,s,:] = cls_tensor\n",
    "        layer_mean_cls = np.mean(cls_matrix,axis=0)\n",
    "        return layer_mean_cls\n",
    "    \n",
    "    else:\n",
    "        token_matrix = np.zeros((n_layers,n_sentences,n_tokens-2,768))\n",
    "        for l,layer in enumerate(bertOut):\n",
    "            l_ix = 0\n",
    "            if l not in layers:\n",
    "                continue\n",
    "            else:\n",
    "                l_ix = l_ix+1\n",
    "            for s,sentence in enumerate(layer):\n",
    "                for t,token in enumerate(sentence[1:-1]): #Exclude [CLS] and [SEP] embeddings\n",
    "                    token_tensor = sentence[t].numpy()\n",
    "                    token_matrix[l_ix-1,s,t,:] = token_tensor\n",
    "        \n",
    "        tokenwise_mean = np.mean(token_matrix,axis=2)\n",
    "        layer_mean_token = np.mean(tokenwise_mean,axis=0)\n",
    "        return layer_mean_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCG():\n",
    "    def __init__(self,k):\n",
    "        \n",
    "        \n",
    "        self.k = k\n",
    "        self.max_score = None\n",
    "        self.summary_score = None\n",
    "        \n",
    "    def __call__(self,labels,summ_index):\n",
    "        \n",
    "        self.max_score = np.sum(sorted(labels)[::-1][:self.k])\n",
    "        self.summary_score = np.sum(labels[summ_index])\n",
    "        \n",
    "        return self.summary_score/self.max_score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = JsonFileTokenizer('example.json',tokenizer,forEval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"dbmdz/bert-base-turkish-cased\",output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens,segments = tok.prepare_for_batch_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating...\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print('Generating...')\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens[0], segments[0])\n",
    "twelve_layers = outputs[2][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = select_layer(twelve_layers,[11],return_cls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 768)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sentence_embeddings.shape[0] == len(tok.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=3,random_state=42)\n",
    "pca = PCA(n_components=2)\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "reduced_cls = tsne.fit_transform(sentence_embeddings)\n",
    "cls_df = pd.DataFrame(reduced_cls,columns=['dim1','dim2'])\n",
    "\n",
    "kmeans.fit_transform(reduced_cls)\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "cluster_df =  pd.DataFrame(cluster_centers,columns=['dim1','dim2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-d3563410978a41b19e65b1430428047d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-d3563410978a41b19e65b1430428047d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-d3563410978a41b19e65b1430428047d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-8fcbe9e89c4fe4643751c92cf7b4ce37\"}, \"mark\": \"circle\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"dim1\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"dim2\"}}}, {\"data\": {\"name\": \"data-44f1a23db48b95ba437ddc2ea3d1e6e3\"}, \"mark\": {\"type\": \"circle\", \"color\": \"red\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"dim1\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"dim2\"}}}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-8fcbe9e89c4fe4643751c92cf7b4ce37\": [{\"dim1\": -222.22430419921875, \"dim2\": -302.7551574707031}, {\"dim1\": 14.72780990600586, \"dim2\": 28.55985450744629}, {\"dim1\": 330.2784423828125, \"dim2\": -30.824050903320312}, {\"dim1\": -1.5797449350357056, \"dim2\": -40.24029541015625}, {\"dim1\": -51.4688720703125, \"dim2\": -96.37459564208984}, {\"dim1\": 56.07677459716797, \"dim2\": 23.640159606933594}, {\"dim1\": 121.68157196044922, \"dim2\": -301.1607360839844}, {\"dim1\": -253.37728881835938, \"dim2\": 64.505615234375}, {\"dim1\": 153.1558380126953, \"dim2\": -245.47523498535156}, {\"dim1\": -169.30398559570312, \"dim2\": -294.6704406738281}, {\"dim1\": -311.3985900878906, \"dim2\": 82.19857025146484}, {\"dim1\": -315.52166748046875, \"dim2\": -306.6146545410156}, {\"dim1\": 314.3687744140625, \"dim2\": 57.165077209472656}, {\"dim1\": 123.24015808105469, \"dim2\": -275.7910461425781}, {\"dim1\": 118.62843322753906, \"dim2\": 24.879045486450195}, {\"dim1\": 275.24127197265625, \"dim2\": 47.917301177978516}, {\"dim1\": 110.0655288696289, \"dim2\": 224.37185668945312}, {\"dim1\": -44.49905776977539, \"dim2\": -162.1643829345703}, {\"dim1\": -296.22772216796875, \"dim2\": -317.3027648925781}, {\"dim1\": 145.02294921875, \"dim2\": -216.92506408691406}, {\"dim1\": 90.2261734008789, \"dim2\": 207.4746856689453}, {\"dim1\": -248.82281494140625, \"dim2\": 98.11595916748047}, {\"dim1\": 108.01254272460938, \"dim2\": -327.5111389160156}, {\"dim1\": 99.63957977294922, \"dim2\": -256.1830749511719}, {\"dim1\": 150.70123291015625, \"dim2\": 31.702472686767578}, {\"dim1\": 363.609619140625, \"dim2\": 35.387840270996094}, {\"dim1\": 228.73663330078125, \"dim2\": 211.3988037109375}, {\"dim1\": 335.5735168457031, \"dim2\": 42.7292366027832}, {\"dim1\": -100.09590911865234, \"dim2\": 64.53142547607422}, {\"dim1\": -276.9105224609375, \"dim2\": 72.01592254638672}, {\"dim1\": 305.89813232421875, \"dim2\": 139.8578643798828}, {\"dim1\": 24.169471740722656, \"dim2\": 52.30337905883789}, {\"dim1\": -78.11095428466797, \"dim2\": 63.36901092529297}, {\"dim1\": 324.7581787109375, \"dim2\": -51.974300384521484}, {\"dim1\": -191.35427856445312, \"dim2\": -311.91644287109375}, {\"dim1\": -175.42266845703125, \"dim2\": -241.94195556640625}, {\"dim1\": -28.994232177734375, \"dim2\": -69.80183410644531}, {\"dim1\": -242.16888427734375, \"dim2\": -7.477255821228027}, {\"dim1\": 205.20196533203125, \"dim2\": 212.27708435058594}, {\"dim1\": 67.25694274902344, \"dim2\": -262.39495849609375}, {\"dim1\": 309.9996643066406, \"dim2\": 103.51039123535156}, {\"dim1\": -272.9571533203125, \"dim2\": 9.913333892822266}, {\"dim1\": 1.4515347480773926, \"dim2\": -7.8548431396484375}, {\"dim1\": 142.28933715820312, \"dim2\": 258.7731628417969}, {\"dim1\": 118.15979766845703, \"dim2\": 263.7585754394531}, {\"dim1\": -180.145263671875, \"dim2\": -271.8764343261719}, {\"dim1\": 78.20281219482422, \"dim2\": 31.024269104003906}], \"data-44f1a23db48b95ba437ddc2ea3d1e6e3\": [{\"dim1\": -221.45712280273438, \"dim2\": -292.439697265625}, {\"dim1\": 116.8585205078125, \"dim2\": -269.3487243652344}, {\"dim1\": 214.33424377441406, \"dim2\": 101.83718872070312}, {\"dim1\": -124.66900634765625, \"dim2\": 10.106658935546875}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(cls_df).mark_circle().encode(x='dim1',y='dim2')  + alt.Chart(cluster_df).mark_circle(color='red').encode(x='dim1',y='dim2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit_transform(sentence_embeddings)\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "cluster_df =  pd.DataFrame(cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "euc_dist = euclidean_distances(sentence_embeddings,cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_indices = []\n",
    "for centroid in range(euc_dist.shape[1]):\n",
    "    sentence_indices.append(np.argmin(euc_dist[:,centroid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sentences = np.array(tok.sentences)[sorted(sentence_indices)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Eyaletlerdeki anketlerin ortalamaları, Amerika’daki son sekiz seçimdir seyreden seçmen davranışı, demografik olarak adayların destek aldığı oy kitlelerinin son sekiz seçim dönemine göre eyalet bazlı hareketleri, diğer iki başkan adayının iki büyük parti adayından eyalet bazında alacağı oylar ve bunun Seçiciler Kurulu’na etkisini göz önünde bulundurdum.',\n",
       "       'Bu sıkkınlık karşısında Trump’a yöneliyorlar ve Hillary ise tam anlamıyla bu statükoyu temsil ediyor.',\n",
       "       'Kanaatimce yukarıdaki söylemlerin seçmeni nasıl etkileyeceği hala tartışmaya açıktır.',\n",
       "       'Hem de şu an Amerika’da hakim olmaya başlayan Trump başkan olmaya yakın ortamının tam da göbeğinde.'],\n",
       "      dtype='<U354')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_score = NDCG(k=4)\n",
    "ndcg_score(tok.labels,sentence_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterEmbeddings(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,k,random_state=None):\n",
    "        super(ClusterEmbeddings,self).__init__()\n",
    "        \n",
    "        self.k = k\n",
    "        self.random_state = random_state\n",
    "        self.cluster_centers = None\n",
    "        self.selected_sentence_indices = []\n",
    "        \n",
    "    def fit_transform(self,X):\n",
    "        \n",
    "        self._X = X\n",
    "        assert self._X.shape[1] == 768 \n",
    "        \n",
    "        kmeans = KMeans(n_clusters=self.k,random_state=self.random_state)\n",
    "        kmeans.fit_transform(X)\n",
    "        self.cluster_centers = kmeans.cluster_centers_\n",
    "        cluster_df =  pd.DataFrame(self.cluster_centers)\n",
    "        \n",
    "        euc_dist = euclidean_distances(self._X,cluster_df)\n",
    "        for centroid in range(euc_dist.shape[1]):\n",
    "            self.selected_sentence_indices.append(np.argmin(euc_dist[:,centroid]))\n",
    "    \n",
    "        \n",
    "        return sorted(self.selected_sentence_indices)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnotatedExtractiveSummarizer():\n",
    "    \n",
    "    \"\"\"\n",
    "        Run summarization and score on annotated data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,tokenizer,model,k=4,layers=[11],use_CLS_token=False,doEval=True,random_state = None):\n",
    "        super(AnnotatedExtractiveSummarizer,self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.doEval = doEval\n",
    "        self.model = model\n",
    "        self.layers = layers\n",
    "        self.use_CLS_token = use_CLS_token\n",
    "        self.k = k\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def summarize(self,jsonPath):\n",
    "        self._jsonTokenizer = JsonFileTokenizer(jsonPath,self.tokenizer,forEval=self.doEval)\n",
    "        self._tokens,self._segments = self._jsonTokenizer.prepare_for_batch_inference()\n",
    "        \n",
    "        self.model.eval()\n",
    "        print('Generating Embeddings...')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(self._tokens[0], self._segments[0])\n",
    "        self._twelve_layers = outputs[2][1:]\n",
    "        \n",
    "        self._sentence_embeddings = select_layer(self._twelve_layers,[11],return_cls=self.use_CLS_token)\n",
    "        \n",
    "        self._cluster_model = ClusterEmbeddings(self.k,self.random_state)\n",
    "        self._selected_indices = self._cluster_model.fit_transform(self._sentence_embeddings)\n",
    "        \n",
    "        selected_sentences = np.array(self._jsonTokenizer.sentences)[self._selected_indices]\n",
    "        return selected_sentences\n",
    "        \n",
    "        \n",
    "    def score(self):\n",
    "        ndcg_score = NDCG(k=self.k)\n",
    "        self._score = ndcg_score(self._jsonTokenizer.labels,self._selected_indices)\n",
    "        \n",
    "        return self._score\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = AnnotatedExtractiveSummarizer(tokenizer,model,k=5,layers=[2,3,8],random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Embeddings...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Eyaletlerdeki anketlerin ortalamaları, Amerika’daki son sekiz seçimdir seyreden seçmen davranışı, demografik olarak adayların destek aldığı oy kitlelerinin son sekiz seçim dönemine göre eyalet bazlı hareketleri, diğer iki başkan adayının iki büyük parti adayından eyalet bazında alacağı oylar ve bunun Seçiciler Kurulu’na etkisini göz önünde bulundurdum.',\n",
       "       'Bilhassa Cumhuriyetçi partili olanlarla fikrimi paylaştığımda Trump’ın seçimi kazanacağını söylediler.',\n",
       "       'Bu sıkkınlık karşısında Trump’a yöneliyorlar ve Hillary ise tam anlamıyla bu statükoyu temsil ediyor.',\n",
       "       'Büyük eksilerle yola çıkmış iki adayın savaşı bu.',\n",
       "       'Tabii ki öyle, ama hala yaptığımız tüm analizlerin değişme ihtimali olmasıyla birlikte Clinton’un seçimi her şeye rağmen 300-330 bandında alacağını düşünüyorum.'],\n",
       "      dtype='<U354')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer.summarize('example.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
