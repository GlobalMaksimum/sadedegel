import pytest

from .context import SimpleTokenizer, BertTokenizer, ICUTokenizer, Text2Doc


@pytest.mark.parametrize('toker, text, tokens_true', [
    (ICUTokenizer, 'ğŸ‘basarili ve kaliteli bir urun .', ['ğŸ‘', 'basarili', 've', 'kaliteli', 'bir', 'urun', '.']),
    (SimpleTokenizer, 'ğŸ‘basarili ve kaliteli bir urun .', ['ğŸ‘', 'basarili', 've', 'kaliteli', 'bir', 'urun', '.']),
    (BertTokenizer, 'ğŸ‘basarili ve kaliteli bir urun .',
     ['ğŸ‘', 'basar', '##ili', 've', 'kaliteli', 'bir', 'ur', '##un', '.'])
])
def test_tokenizer_emoji(text, tokens_true, toker):
    tokenizer = toker(emoji=True)
    tokens_pred = tokenizer(text)
    assert tokens_pred == tokens_true


@pytest.mark.parametrize('toker, text, tokens_true', [
    (ICUTokenizer, 'Ã§ok gÃ¼zel kÃ¼tÃ¼phane olmuÅŸ @sadedegel', ['Ã§ok', 'gÃ¼zel', 'kÃ¼tÃ¼phane', 'olmuÅŸ', '@sadedegel']),
    (SimpleTokenizer, 'Ã§ok gÃ¼zel kÃ¼tÃ¼phane olmuÅŸ @sadedegel', ['Ã§ok', 'gÃ¼zel', 'kÃ¼tÃ¼phane', 'olmuÅŸ', '@sadedegel']),
    (BertTokenizer, 'Ã§ok gÃ¼zel kÃ¼tÃ¼phane olmuÅŸ @sadedegel', ['Ã§ok', 'gÃ¼zel', 'kÃ¼tÃ¼phane', 'olmuÅŸ', '@sadedegel'])
])
def test_tokenizer_mention(text, tokens_true, toker):
    tokenizer = toker(mention=True)
    tokens_pred = tokenizer(text)
    assert tokens_pred == tokens_true


@pytest.mark.parametrize('toker, text, tokens_true', [
    (ICUTokenizer, 'aÄŸaÃ§lar yanmasÄ±n! #yeÅŸiltÃ¼rkiye', ['aÄŸaÃ§lar', 'yanmasÄ±n', '!', '#yeÅŸiltÃ¼rkiye']),
    (SimpleTokenizer, 'aÄŸaÃ§lar yanmasÄ±n! #yeÅŸiltÃ¼rkiye', ['aÄŸaÃ§lar', 'yanmasÄ±n', '#yeÅŸiltÃ¼rkiye']),
    (BertTokenizer, 'aÄŸaÃ§lar yanmasÄ±n! #yeÅŸiltÃ¼rkiye', ['aÄŸaÃ§lar', 'yanma', '##sÄ±n', '!', '#yeÅŸiltÃ¼rkiye'])
])
def test_tokenizer_hashtag(text, tokens_true, toker):
    tokenizer = toker(hashtag=True)
    tokens_pred = tokenizer(text)
    assert tokens_pred == tokens_true


# Text2Doc Tests

@pytest.mark.parametrize('toker, text, tokens_true', [
    ('icu', ['ğŸ‘basarili ve kaliteli bir urun .'], ['ğŸ‘', 'basarili', 've', 'kaliteli', 'bir', 'urun', '.']),
    ('simple', ['ğŸ‘basarili ve kaliteli bir urun .'], ['ğŸ‘', 'basarili', 've', 'kaliteli', 'bir', 'urun', '.']),
    ('bert', ['ğŸ‘basarili ve kaliteli bir urun .'],
     ['ğŸ‘', 'basar', '##ili', 've', 'kaliteli', 'bir', 'ur', '##un', '.'])
])
def test_t2d_emoji(text, tokens_true, toker):
    tokenizer = Text2Doc(tokenizer=toker, emoji=True)
    tokens_pred = tokenizer.transform(text)
    assert tokens_pred[0].tokens == tokens_true


@pytest.mark.parametrize('toker, text, tokens_true', [
    ('icu', ['Ã§ok gÃ¼zel kÃ¼tÃ¼phane olmuÅŸ @sadedegel'], ['Ã§ok', 'gÃ¼zel', 'kÃ¼tÃ¼phane', 'olmuÅŸ', '@sadedegel']),
    ('simple', ['Ã§ok gÃ¼zel kÃ¼tÃ¼phane olmuÅŸ @sadedegel'], ['Ã§ok', 'gÃ¼zel', 'kÃ¼tÃ¼phane', 'olmuÅŸ', '@sadedegel']),
    ('bert', ['Ã§ok gÃ¼zel kÃ¼tÃ¼phane olmuÅŸ @sadedegel'], ['Ã§ok', 'gÃ¼zel', 'kÃ¼tÃ¼phane', 'olmuÅŸ', '@sadedegel'])
])
def test_t2d_mention(text, tokens_true, toker):
    tokenizer = Text2Doc(tokenizer=toker, mention=True)
    tokens_pred = tokenizer.transform(text)
    assert tokens_pred[0].tokens == tokens_true


@pytest.mark.parametrize('toker, text, tokens_true', [
    ('icu', ['aÄŸaÃ§lar yanmasÄ±n! #yeÅŸiltÃ¼rkiye'], ['aÄŸaÃ§lar', 'yanmasÄ±n', '!', '#yeÅŸiltÃ¼rkiye']),
    ('simple', ['aÄŸaÃ§lar yanmasÄ±n! #yeÅŸiltÃ¼rkiye'], ['aÄŸaÃ§lar', 'yanmasÄ±n', '#yeÅŸiltÃ¼rkiye']),
    ('bert', ['aÄŸaÃ§lar yanmasÄ±n! #yeÅŸiltÃ¼rkiye'], ['aÄŸaÃ§lar', 'yanma', '##sÄ±n', '!', '#yeÅŸiltÃ¼rkiye'])
])
def test_t2d_hashtag(text, tokens_true, toker):
    tokenizer = Text2Doc(tokenizer=toker, hashtag=True)
    tokens_pred = tokenizer.transform(text)
    assert tokens_pred[0].tokens == tokens_true
