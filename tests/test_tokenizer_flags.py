import pytest

from .context import SimpleTokenizer, BertTokenizer, ICUTokenizer


@pytest.mark.parametrize('toker, text, tokens_true', [
    (ICUTokenizer, 'ğŸ‘basarili ve kaliteli bir urun .', ['ğŸ‘', 'basarili', 've', 'kaliteli', 'bir', 'urun', '.']),
    (SimpleTokenizer, 'ğŸ‘basarili ve kaliteli bir urun .', ['ğŸ‘', 'basarili', 've', 'kaliteli', 'bir', 'urun', '.']),
    (BertTokenizer, 'ğŸ‘basarili ve kaliteli bir urun .',
     ['ğŸ‘', 'basar', '##ili', 've', 'kaliteli', 'bir', 'ur', '##un', '.'])
])
def test_tokenizer_emoji(text, tokens_true, toker):
    it = toker(emoji=True)
    tokens_pred = it._tokenize(text)
    assert tokens_pred == tokens_true


@pytest.mark.parametrize('toker, text, tokens_true', [
    (ICUTokenizer, 'Ã§ok gÃ¼zel kÃ¼tÃ¼phane olmuÅŸ @sadedegel', ['Ã§ok', 'gÃ¼zel', 'kÃ¼tÃ¼phane', 'olmuÅŸ', '@sadedegel']),
    (SimpleTokenizer, 'Ã§ok gÃ¼zel kÃ¼tÃ¼phane olmuÅŸ @sadedegel', ['Ã§ok', 'gÃ¼zel', 'kÃ¼tÃ¼phane', 'olmuÅŸ', '@sadedegel']),
    (BertTokenizer, 'Ã§ok gÃ¼zel kÃ¼tÃ¼phane olmuÅŸ @sadedegel', ['Ã§ok', 'gÃ¼zel', 'kÃ¼tÃ¼phane', 'olmuÅŸ', '@sadedegel'])
])
def test_tokenizer_mention(text, tokens_true, toker):
    it = toker(mention=True)
    tokens_pred = it._tokenize(text)
    assert tokens_pred == tokens_true


@pytest.mark.parametrize('toker, text, tokens_true', [
    (ICUTokenizer, 'aÄŸaÃ§lar yanmasÄ±n! #yeÅŸiltÃ¼rkiye', ['aÄŸaÃ§lar', 'yanmasÄ±n', '!', '#yeÅŸiltÃ¼rkiye']),
    (SimpleTokenizer, 'aÄŸaÃ§lar yanmasÄ±n! #yeÅŸiltÃ¼rkiye', ['aÄŸaÃ§lar', 'yanmasÄ±n', 'yeÅŸiltÃ¼rkiye']),
    (BertTokenizer, 'aÄŸaÃ§lar yanmasÄ±n! #yeÅŸiltÃ¼rkiye', ['aÄŸaÃ§lar', 'yanma', '##sÄ±n', '!', '#yeÅŸiltÃ¼rkiye'])
])

def test_tokenizer_hashtag(text, tokens_true, toker):
    it = toker(hashtag=True)
    tokens_pred = it._tokenize(text)
    assert tokens_pred == tokens_true
