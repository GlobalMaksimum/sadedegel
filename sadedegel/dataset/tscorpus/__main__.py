import os.path
from pathlib import Path
import sys
from itertools import tee

from smart_open import open

import click

import boto3

from loguru import logger

from ._core import load_tokenization_tokenized, load_tokenization_raw, CORPUS_SIZE, tarballs

from rich.console import Console
from rich.table import Table

from sadedegel.config import tokenizer_context

console = Console()

logger.disable("sadedegel")


@click.group(help="Tokenization Dataset Commandline")
def cli():
    pass


@cli.command()
@click.option("--access-key", help="Access Key ID to download dataset.", prompt=True,
              default=lambda: os.environ.get('sadedegel_access_key', ''))
@click.option("--secret-key", help="Secret Key ID to download dataset.", prompt=True,
              default=lambda: os.environ.get('sadedegel_secret_key', ''))
@click.option("--data_home", '-d', help="Data home directory", default="~/.sadedegel_data")
def download(access_key, secret_key, data_home):
    """Download tokenization corpus from cloud with your key."""

    data_home = Path(os.path.expanduser(data_home)) / 'tscorpus'
    console.print(f"Data directory for tokenization data {data_home}")

    transport_params = {
        'session': boto3.Session(aws_access_key_id=access_key,
                                 aws_secret_access_key=secret_key),
        'resource_kwargs': {
            'endpoint_url': 'https://storage.googleapis.com',
        }
    }

    for fmt in ['raw', 'tokenized']:
        (data_home / fmt).mkdir(parents=True, exist_ok=True)

        with console.status(f"[bold green]Downloading {fmt}...") as status:
            for tarball in tarballs:
                url = f"s3://sadedegel/dataset/tscorpus/{fmt}/{tarball}"

                with open(url, 'rb', transport_params=transport_params) as fp, open(data_home / fmt / tarball,
                                                                                    "wb") as wp:
                    wp.write(fp.read())

                console.log(f"{fmt}/{tarball} complete.")


@cli.command()
def validate():
    raw = load_tokenization_raw()
    tok = load_tokenization_tokenized()

    with console.status("[bold yellow]Validating tscorpus"):

        raw_clone, raw = tee(raw, 2)
        tok_clone, tok = tee(tok, 2)

        n_doc_raw = sum(1 for _ in raw_clone)
        n_doc_tok = sum(1 for _ in tok_clone)

        if n_doc_raw != n_doc_tok:
            console.log("Cardinality check [red]FAILED[/red]")
            console.log(f"|TsCorpus (raw)|: {n_doc_raw}")
            console.log(f"|TsCorpus (tokenized)|: {n_doc_tok}")

            sys.exit(1)
        else:
            if n_doc_raw == CORPUS_SIZE:
                console.log("Cardinality check [yellow]DONE[/yellow]")
                n_document = CORPUS_SIZE
            else:
                console.log("Cardinality check [red]FAILED[/red]")
                console.log(f"|TsCorpus (raw)| : {n_doc_raw}")
                sys.exit(1)

        count = 0
        for i, (d1, d2) in enumerate(zip(raw, tok)):
            if d1['id'] != d2['id']:
                console.log("Document order check [red]FAILED[/red]")
                console.log(f"{i}th document (0-based index) raw#:{d1['id']}, tokenized#:{d2['id']}")

                sys.exit(1)

            count += 1
        console.log("Document order check [yellow]DONE[/yellow]")

        if count != n_document:
            console.log(f"[red]Corpus cardinality is {count}/{n_document}. 302936 is expected.")
            sys.exit(1)
        else:

            console.log("TsCorpus is [green]VALID[/green]")


@cli.command()
@click.option("--sample_size", '-s', help="Sampling size.", default=10)
@click.option("--tokenizer", '-t', help="Tokenizer", default="simple")
def diff(sample_size, tokenizer):
    """Git like diff tool to compare sentence generated by our tokenizer vs actual list of sentences."""
    console.log("Loading corpus...")

    raw = load_tokenization_raw()
    tok = load_tokenization_tokenized()

    # Collect Raw and Tokenized Data
    raw_samples, tok_samples, doc_ids, pred_samples = [], [], [], []

    for i, (raw_doc, tokenized_doc) in enumerate(zip(raw, tok)):
        raw_samples.append(raw_doc['text'])
        tok_samples.append(tokenized_doc['tokens'])
        doc_ids.append(raw_doc['id'])
        if i == sample_size:
            break

    # Tokenize Raw Data to obtain Predictions.
    with tokenizer_context(tokenizer) as Doc2:
        for document in raw_samples:
            d = Doc2(document)
            tokens = [tok for tok in d.tokens if not tok.startswith('##')]
            pred_samples.append(tokens)

    # Calculate diff between True(TS Tokens) and Predicted
    for sample_ix in range(sample_size):
        if tok_samples[sample_ix] != pred_samples[sample_ix]:

            # Title
            console.rule(f"Document {doc_ids[sample_ix]}")

            # False Negatives - Missed tokens
            for s_true in tok_samples[sample_ix]:
                if s_true not in pred_samples[sample_ix]:
                    console.log(f"[green]+ {s_true}[/green]")
            # False Positives - Extra tokens
            for s_pred in pred_samples[sample_ix]:
                if s_pred not in tok_samples[sample_ix]:
                    console.log(f"[red]- {s_pred}[/red]")


if __name__ == "__main__":
    cli()
