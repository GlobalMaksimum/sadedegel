from sadedegel.dataset import load_raw_corpus, load_sentence_corpus, file_paths
from sadedegel.tokenize import NLTKPunctTokenizer, RegexpSentenceTokenizer
from sadedegel import Doc
from sadedegel.bblock.util import flatten, is_eos
from sadedegel.ml import create_model, save_model
from sklearn.model_selection import cross_val_score

from difflib import Differ

from rich.console import Console
from rich.table import Table
from collections import Counter
from ..bblock.util import deprecate

console = Console()

import click
from typing import List


@click.group(help="sadedeGel SBD commandline")
def tokenize():
    deprecate("sadedegel-sbd is deprecated", (0, 21, 0), "Use [yellow]sadedegel sbd[/yellow] instead.")


def iou_eval(name: str, y_true: List[List[str]], y_pred: List[List[str]], paths=None):
    micro_i, micro_u, iou = 0, 0, 0

    for idx, (seq1, seq2) in enumerate(zip(y_true, y_pred)):
        s1 = Counter(map(lambda x: x.strip(), seq1))
        s2 = Counter(map(lambda x: x.strip(), seq2))

        i = sum((s1 & s2).values())
        u = sum((s1 | s2).values())

        micro_i += i
        micro_u += u

        delta = i / u

        if paths:
            click.secho(f"Lost {1.0 - delta:.2f} because of {paths[idx]}", fg="yellow")

        iou += delta

    click.secho(f"IoU score for {name}")

    click.secho(f"Micro IoU: {micro_i / micro_u:.4f}".rjust(25))
    click.secho(f"Macro IoU: {iou / len(y_true):.4f}".rjust(25))


@tokenize.command()
@click.option("-v", count=True, help="verbosity")
def evaluate(v):
    """Evaluate IoU metric for different SBD algorithms over our stock dataset."""
    click.secho("Loading corpus...")
    raw, sents = load_raw_corpus(False), load_sentence_corpus()

    nltk = NLTKPunctTokenizer()
    reg = RegexpSentenceTokenizer()

    y_pred = [nltk(doc) for doc in raw]
    y_true = [doc['sentences'] for doc in sents]

    iou_eval("NLTKPunctTokenizer", y_true, y_pred, file_paths() if v > 0 else None)

    y_pred = [reg(doc) for doc in raw]

    iou_eval("RegexpSentenceTokenizer", y_true, y_pred, file_paths() if v > 0 else None)

    y_pred = [[s.text for s in Doc(doc)] for doc in raw]

    iou_eval("MLBasedTokenizer", y_true, y_pred, file_paths() if v > 0 else None)


@tokenize.command()
@click.option('-v', '--verbose', count=True)
def diff(verbose):
    """Git like diff tool to compare sentence generated by our tokenizer vs actual list of sentences."""
    click.secho("Loading corpus...")
    raw, sents = load_raw_corpus(False), load_sentence_corpus()

    y_true = [doc['sentences'] for doc in sents]

    y_pred = [[str(s) for s in Doc(doc)] for doc in raw]

    paths = file_paths()

    differ = Differ()

    for t, p, f in zip(y_true, y_pred, paths):

        table = Table(show_header=True, header_style="bold magenta", show_edge=False)
        table.add_column("true", style="dim", width=100)
        table.add_column("predict", style="dim", width=100)

        table.columns[0].style = "green"
        table.columns[1].style = "red"

        ndiff = 0
        match = 0
        for sent in differ.compare(p, t):
            if sent.startswith('+'):
                if match > 0 and verbose > 0:
                    table.add_row(f"[blue]{match} sentences...[/blue]", f"[blue]{match} sentences...[/blue]")
                    match = 0

                table.add_row(sent[2:], "")
                ndiff += 1
            elif sent.startswith('-'):
                if match > 0 and verbose > 0:
                    table.add_row(f"[blue]{match} sentences...[/blue]", f"[blue]{match} sentences...[/blue]")
                    match = 0

                table.add_row("", sent[2:])
                ndiff += 1
            else:
                match += 1

        if match > 0 and verbose > 0:
            table.add_row(f"[blue]{match} sentences...[/blue]", f"[blue]{match} sentences...[/blue]")

        if ndiff > 0:
            console.print(f)
            console.print(table)
            console.print(f"[blue]{len(t)} sentences...[/blue]")
            console.print()


@tokenize.command()
def build():
    """Build a ML based SBD"""

    raw_corpus = load_raw_corpus(False)
    sent_corpus = load_sentence_corpus(False)

    features = flatten([[span.span_features() for span in Doc(raw).spans] for raw in raw_corpus])
    y = flatten(
        [[is_eos(span, sent['sentences']) for span in Doc(raw).spans] for raw, sent in
         zip(raw_corpus, sent_corpus)])

    if len(features) != len(y):
        raise Exception(f"Sanity check failed feature list length {len(features)} whereas target list length {len(y)}.")

    sbd_model = create_model()

    scores = cross_val_score(sbd_model.pipeline, features, y, scoring="f1")

    for i, score in enumerate(scores):
        click.secho(f"Fold {i + 1}: {score:.4f}", fg="yellow")

    sbd_model.fit(features, y)

    click.secho("\nTop 10 Features")
    feature_importance = sbd_model.pipeline.steps[1][1].feature_importances_
    for idx in list(reversed(feature_importance.argsort()))[:20]:
        click.secho(f"    {sbd_model.pipeline.steps[0][1].feature_names_[idx]}: {feature_importance[idx]:.4f}",
                    fg="yellow")

    save_model(sbd_model)


if __name__ == '__main__':
    tokenize()
